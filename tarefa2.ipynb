{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Imports",
   "id": "47887e73521ce6b9"
  },
  {
   "cell_type": "code",
   "id": "f629e88f-7461-40b0-a56f-0759b4f4d0dc",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### > Tarefa 2\n",
    "Tivemos que usar o Selenium pois o site dos top 250 filmes não carregava mais que 25 filmes de fato, além disso, usamos uma \"roupagem\" de navegador para garantir o acesso no site do IMDB."
   ],
   "id": "e0790b975f6952fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.1 - Crawler\n",
    "\n",
    "Tentamos realizar as tarefas 2.1 e 2.2 de forma diferente a como foi proposto no enunciado, separando o processo entre um crawler e uma etapa de raspagem em cima dos htmls dos filmes (semelhante a como foi feito a tarefa 1.0), pois parecia ser menos estressante pro site do IMDB quanto as requisicoes."
   ],
   "id": "5d057009b85295ba"
  },
  {
   "cell_type": "code",
   "id": "067051d2-ba92-4842-ad5e-ad59af64506d",
   "metadata": {},
   "source": [
    "url_base = \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\"\n",
    "\n",
    "fake_agent = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9ff4f6c-cd1f-48e1-b663-c49e0766bf94",
   "metadata": {},
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(url_base)\n",
    "time.sleep(2)\n",
    "\n",
    "main_page = driver.page_source\n",
    "driver.quit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e02fe5fb-4a40-4033-b98b-ab9a561c2496",
   "metadata": {},
   "source": [
    "soup = BeautifulSoup(main_page)\n",
    "lista_filmes = soup.find(\"ul\",{\"class\":\"ipc-metadata-list ipc-metadata-list--dividers-between sc-a1e81754-0 dHaCOW compact-list-view ipc-metadata-list--base\"})\n",
    "\n",
    "top_250_page = lista_filmes.find_all(\"a\",{\"class\":\"ipc-title-link-wrapper\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8bdc9ec3-c265-4554-b0cd-996558873a24",
   "metadata": {},
   "source": [
    "for i in range(250):\n",
    "    req_aux = requests.get(\"https://www.imdb.com/\" + top_250_page[i].get(\"href\"),headers=fake_agent)\n",
    "    with open(f\"tarefa2/filmes/{i}.html\", \"w\", encoding=\"utf-8\") as arq:\n",
    "        arq.write(req_aux.text)\n",
    "        time.sleep(0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.2 - Scraping dos filmes",
   "id": "cb5c15c10cd0a40c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Serie de funcoes lambda para raspar os atributos dos filmes do top 250\n",
    "\n",
    "nome_filme = lambda x: x.select_one(\"h1 > span[class='hero__primary-text']\")\n",
    "\n",
    "def duracao_filme(obj_soup,func):\n",
    "    aux = func(obj_soup)\n",
    "    if len(aux) > 2 and aux[2] is not None:\n",
    "        return aux[2].text\n",
    "    elif len(aux) > 1 and aux[1] is not None:\n",
    "        return aux[1].text\n",
    "    else:\n",
    "        return \"Não informada\"\n",
    "\n",
    "duracao = lambda x: x.select(\"ul[class='ipc-inline-list ipc-inline-list--show-dividers sc-ec65ba05-2 joVhBE baseAlt'] > li\")\n",
    "\n",
    "url_poster = lambda x: x.select(\"head > meta[property='og:url']\")[0].get('content')\n",
    "\n",
    "link_imagem = lambda x: x.select_one(\"img.ipc-image\")[\"src\"]\n",
    "\n",
    "nota_imdb = lambda x: x.select_one(\"div[data-testid='hero-rating-bar__aggregate-rating__score'] > span[class='sc-eb51e184-1 ljxVSS']\")\n",
    "\n",
    "popularidade = lambda x: x.find(\"div\", {\"data-testid\":\"hero-rating-bar__popularity__score\"})\n",
    "\n",
    "ator = lambda x: x.select(\"div.ipc-sub-grid.ipc-sub-grid--page-span-2.ipc-sub-grid--wraps-at-above-l.ipc-shoveler__grid a[data-testid='title-cast-item__actor']\")\n",
    "\n",
    "personagem = lambda x: x.select(\"div[class = 'title-cast-item__characters-list'] > ul[class='ipc-inline-list ipc-inline-list--no-wrap ipc-inline-list--inline base'] > li[role='presentation'] > a > span\")"
   ],
   "id": "6788cef0f3dd852c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1000f86-2215-49a1-945e-4010283d94a4",
   "metadata": {},
   "source": [
    "# Aplica as funcoes lambda em cada pagina resgatada pelo crawler para realizar o scraping, empilhando tudo no dicionario de filmes (nosso futuro json)\n",
    "dict_filmes = {}\n",
    "\n",
    "for i in range(250):\n",
    "    filme = {}\n",
    "\n",
    "    with open(f\"tarefa2/filmes/{i}.html\", \"r\", encoding=\"utf-8\") as arq:\n",
    "        soup_aux = BeautifulSoup(arq)\n",
    "\n",
    "        filme[\"titulo\"] = nome_filme(soup_aux).text\n",
    "\n",
    "        filme[\"duracao\"] = duracao_filme(obj_soup=soup_aux, func=duracao)\n",
    "\n",
    "        filme[\"url_poster\"] = url_poster(soup_aux)\n",
    "\n",
    "        filme[\"img_poster\"] = link_imagem(soup_aux)\n",
    "\n",
    "        filme[\"nota_imdb\"] = nota_imdb(soup_aux).text\n",
    "\n",
    "        # Alguns filmes nao tem medida de popularidade, pra esses casos preenchemos como 'nao informada'\n",
    "        try:\n",
    "            filme[\"popularidade\"] = popularidade(soup_aux).text\n",
    "        except Exception:\n",
    "            filme[\"popularidade\"] = \"Não informada\"\n",
    "\n",
    "        # Crio um subdicionario para o elenco\n",
    "        elenco = {}\n",
    "        ator_personagem = zip([i.text for i in ator(soup_aux)], [i.text for i in personagem(soup_aux)]) # Converto o elemento das listas para o texto do html antes de zippar\n",
    "        \n",
    "        for j, pessoa in enumerate(ator_personagem):\n",
    "            participante = {\"ator\": pessoa[0], \"personagem\": pessoa[1]}\n",
    "            elenco[f\"pessoa{j}\"] = participante\n",
    "        \n",
    "        filme[\"elenco\"] = elenco\n",
    "\n",
    "    dict_filmes[f\"filme{i}\"] = filme"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f20cd15e-4082-4d74-b1f7-ca5259a9712f",
   "metadata": {},
   "source": [
    "# Salva em um JSON\n",
    "with open(\"tarefa2/filmes.json\", \"w\", encoding=\"utf-8\") as arq:\n",
    "    json.dump(dict_filmes, arq, ensure_ascii=False, indent=4)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
