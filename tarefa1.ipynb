{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd2ad53",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "fe82df7b",
   "metadata": {},
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b33fba9-2acd-47be-ae1b-a357bfe22079",
   "metadata": {},
   "source": [
    "### > Tarefa 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec8dc23-ceeb-4bc0-91a7-d044cc7b18bd",
   "metadata": {},
   "source": [
    "#### 1.1 - Crawler"
   ]
  },
  {
   "cell_type": "code",
   "id": "0e1f25b9",
   "metadata": {},
   "source": [
    "# Função que salva a nossa página html na pasta \"paginas\" junto ao número da página\n",
    "def salva_pagina(num_pag, html):\n",
    "    with open(f\"tarefa1/paginas/{num_pag}.html\", \"w\", encoding=\"utf-8\") as arq:\n",
    "        arq.write(html)\n",
    "\n",
    "# Função que busca a próxima página pelo botão \"Next >\"\n",
    "def encontra_proxima_pag(req):\n",
    "    pag_bs = BeautifulSoup(req.text)\n",
    "    url = pag_bs.find('a', string=[\"Next >\"]).get('href')\n",
    "    \n",
    "    return \"http://127.0.0.1:8000\" + url\n",
    "\n",
    "# Define o nosso crawler, que navega entre as páginas recursivamente salvando o .html delas\n",
    "def crawler(url, num_pag=0):\n",
    "    #print(url)\n",
    "    req = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    link_paises = [i.get('href') for i in soup.select(\"div[id='results'] > table > tbody > tr > td > div > a\")]\n",
    "\n",
    "    for pais in link_paises:\n",
    "        req_pais = requests.get(\"http://127.0.0.1:8000\" + pais)\n",
    "        salva_pagina(num_pag, req_pais.text)\n",
    "        num_pag+=1\n",
    "\n",
    "    try:\n",
    "        return crawler(encontra_proxima_pag(req), num_pag)\n",
    "    except:\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7267564a-cc2e-4dc5-a837-dc8df55b066a",
   "metadata": {},
   "source": [
    "# Executando o Crawler\n",
    "\n",
    "url = \"http://127.0.0.1:8000/places/default/index/0\"\n",
    "crawler(url)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c06e79f3-f747-4e6c-9cd8-ef7faf6e7072",
   "metadata": {},
   "source": [
    "#### 1.2 - \"Page\" Scraper"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d275b58-35cc-4bb5-9246-bcc3e8d218b0",
   "metadata": {},
   "source": [
    "# Função que retorna uma página salva em meio aos arquivos que o Crawler coletou\n",
    "def abre_pagina(num_pag):\n",
    "    with open(f\"tarefa1/paginas/{num_pag}.html\", \"r\", encoding=\"utf-8\") as arq:\n",
    "        return arq.readlines()\n",
    "\n",
    "# Função que abstrai o padrão de tag da maior parte dos atributos do nosso scraping\n",
    "def cata_atributo(bs, id):\n",
    "    return bs.find('tr', id=id).find('td', class_='w2p_fw')\n",
    "\n",
    "# Faz a \"raspagem\" dos dados, abrindo pelos hrefs referentes aos países daquela página e empilhando os atributos coletados de cada país\n",
    "def scrap_pagina(pag):\n",
    "    pais_bs = BeautifulSoup(str(pag))\n",
    "\n",
    "    # String que vai guardar os atributos dos países disponíveis nessa página\n",
    "    scrap_paises = \"\"\n",
    "\n",
    "    # Sub-processo para pegar o nome completo do continente, acessando a página do continente pela sua sigla\n",
    "    conti_url = \"http://127.0.0.1:8000\" + cata_atributo(pais_bs, 'places_continent__row').find('a').get('href')\n",
    "    conti_req = requests.get(conti_url)\n",
    "    \n",
    "    conti_bs = BeautifulSoup(conti_req.text)\n",
    "\n",
    "    # Atributos\n",
    "    dt_hr_update = str(datetime.now())[:19].replace(' ', 'T') # Formata o nosso timestamp de forma \"mais light\"\n",
    "    nm_pais = cata_atributo(pais_bs, 'places_country__row').text\n",
    "    nm_capital = cata_atributo(pais_bs, 'places_capital__row').text\n",
    "    nm_currency = cata_atributo(pais_bs, 'places_currency_name__row').text\n",
    "    nm_continente = conti_bs.find('section', id='main').find('h2').text\n",
    "\n",
    "    # Agrupa os atributos na string de scrap, em formato compatível com o .csv\n",
    "    scrap_paises += dt_hr_update+\";\"+nm_pais+\";\"+nm_capital+\";\"+nm_currency+\";\"+nm_continente+\"\\n\"\n",
    "\n",
    "    try:\n",
    "        return scrap_paises + crawler(encontra_proxima_pag(req), num_pag)\n",
    "    except:\n",
    "        return scrap_paises"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "06857dfe-8aff-4439-ba1e-63ac83ee970a",
   "metadata": {},
   "source": [
    "# Gerando o dataset\n",
    "dataset_paises = 'dt_hr_update;nm_pais;nm_capital;nm_currency;nm_continente\\n'\n",
    "for pag in range(252):\n",
    "    dataset_paises += scrap_pagina(abre_pagina(pag))\n",
    "\n",
    "# Remove a última linha (vazia)\n",
    "dataset_paises = dataset_paises[:-1]\n",
    "\n",
    "# Salva a variavel str como dataset em formato csv\n",
    "with open(\"tarefa1/dataset_paises.csv\", \"w\", encoding=\"utf-8\") as arq:\n",
    "    arq.write(dataset_paises)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.3 - Monitor de arquivos",
   "id": "525013b005fe458a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Executando o Crawler novamente em busca de atualizacoes\n",
    "\n",
    "url = \"http://127.0.0.1:8000/places/default/index/0\"\n",
    "crawler(url)\n",
    "\n",
    "# Abre o dataset previamente salvo\n",
    "dataset_paises = []\n",
    "with open(\"tarefa1/dataset_paises.csv\", \"r\", encoding=\"utf-8\") as arq:\n",
    "    dataset_paises = arq.readlines()\n",
    "\n",
    "# Copia o dataset para comparacoes futuras\n",
    "dataset_original = dataset_paises.copy()\n",
    "\n",
    "# Itera em cima das paginas que estao sendo checadas, comparando/modificando com a versao presente no dataset\n",
    "for pag in range(252):\n",
    "    pagina_staged = scrap_pagina(abre_pagina(pag))\n",
    "    pagina_dataset = dataset_paises[pag+1]\n",
    "\n",
    "    # Compara o arquivo mais recente com os dados do dataset (sem comparar a data/hora de update ou '\\n')\n",
    "    if pagina_dataset.split('\\n')[0].split(';')[1:] != pagina_staged.split('\\n')[0].split(';')[1:]:\n",
    "        dataset_paises[pag+1] = pagina_staged\n",
    "\n",
    "# Salva o dataset caso atualizacoes tenham acontecido\n",
    "if dataset_paises != dataset_original:\n",
    "    dataset_atualizado = ''\n",
    "\n",
    "    # Convertendo a lista em string\n",
    "    for i in dataset_paises:\n",
    "        dataset_atualizado += i\n",
    "\n",
    "    with open(\"tarefa1/dataset_paises.csv\", \"w\", encoding=\"utf-8\") as arq:\n",
    "        arq.write(dataset_atualizado)"
   ],
   "id": "b6f2c78803508d64",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
